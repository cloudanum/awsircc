{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "An example of how to use DataFrame for ML. Run with::\n",
    "    bin/spark-submit examples/src/main/python/ml/dataframe_example.py <input_path>\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) > 2:\n",
    "        print(\"Usage: dataframe_example.py <libsvm file>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "    elif len(sys.argv) == 2:\n",
    "        input_path = sys.argv[1]\n",
    "    else:\n",
    "        input_path = \"data/mllib/sample_libsvm_data.txt\"\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"DataFrameExample\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load an input file\n",
    "    print(\"Loading LIBSVM file with UDT from \" + input_path + \".\")\n",
    "    df = spark.read.format(\"libsvm\").load(input_path).cache()\n",
    "    print(\"Schema from LIBSVM:\")\n",
    "    df.printSchema()\n",
    "    print(\"Loaded training data as a DataFrame with \" +\n",
    "          str(df.count()) + \" records.\")\n",
    "\n",
    "    # Show statistical summary of labels.\n",
    "    labelSummary = df.describe(\"label\")\n",
    "    labelSummary.show()\n",
    "\n",
    "    # Convert features column to an RDD of vectors.\n",
    "    features = MLUtils.convertVectorColumnsFromML(df, \"features\") \\\n",
    "        .select(\"features\").rdd.map(lambda r: r.features)\n",
    "    summary = Statistics.colStats(features)\n",
    "    print(\"Selected features column with average values:\\n\" +\n",
    "          str(summary.mean()))\n",
    "\n",
    "    # Save the records in a parquet file.\n",
    "    tempdir = tempfile.NamedTemporaryFile(delete=False).name\n",
    "    os.unlink(tempdir)\n",
    "    print(\"Saving to \" + tempdir + \" as Parquet file.\")\n",
    "    df.write.parquet(tempdir)\n",
    "\n",
    "    # Load the records back.\n",
    "    print(\"Loading Parquet file with UDT from \" + tempdir)\n",
    "    newDF = spark.read.parquet(tempdir)\n",
    "    print(\"Schema from Parquet:\")\n",
    "    newDF.printSchema()\n",
    "    shutil.rmtree(tempdir)\n",
    "\n",
    "    spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
