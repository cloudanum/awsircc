{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#\n",
        "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
        "# contributor license agreements.  See the NOTICE file distributed with\n",
        "# this work for additional information regarding copyright ownership.\n",
        "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
        "# (the \"License\"); you may not use this file except in compliance with\n",
        "# the License.  You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "\n",
        "\"\"\"\n",
        "Pipeline Example.\n",
        "\"\"\"\n",
        "\n",
        "# $example on$\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "# $example off$\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession\\\n",
        "        .builder\\\n",
        "        .appName(\"PipelineExample\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # $example on$\n",
        "    # Prepare training documents from a list of (id, text, label) tuples.\n",
        "    training = spark.createDataFrame([\n",
        "        (0, \"a b c d e spark\", 1.0),\n",
        "        (1, \"b d\", 0.0),\n",
        "        (2, \"spark f g h\", 1.0),\n",
        "        (3, \"hadoop mapreduce\", 0.0)\n",
        "    ], [\"id\", \"text\", \"label\"])\n",
        "\n",
        "    # Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "    hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "    lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
        "    pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
        "\n",
        "    # Fit the pipeline to training documents.\n",
        "    model = pipeline.fit(training)\n",
        "\n",
        "    # Prepare test documents, which are unlabeled (id, text) tuples.\n",
        "    test = spark.createDataFrame([\n",
        "        (4, \"spark i j k\"),\n",
        "        (5, \"l m n\"),\n",
        "        (6, \"spark hadoop spark\"),\n",
        "        (7, \"apache hadoop\")\n",
        "    ], [\"id\", \"text\"])\n",
        "\n",
        "    # Make predictions on test documents and print columns of interest.\n",
        "    prediction = model.transform(test)\n",
        "    selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "    for row in selected.collect():\n",
        "        rid, text, prob, prediction = row\n",
        "        print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))\n",
        "    # $example off$\n",
        "\n",
        "    spark.stop()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}